#!/usr/bin/env python
"""Kraken Hadoop remote control

Usage:
  khadoop [options] <service> <action> [NODES...]
  khadoop [options] logs [NODES...]

Options:
  -h --help                 show this help message and exit
  -g GROUP --group GROUP    set the dsh group for kraken nodes [default: k]
  --no-drop-sig1            do not squelch 'Killed by signal 1' messages

Examples:
  
  Get the status of the namenode on an01:
    khadoop namenode status an01
  
  Restart all hadoop services across all nodes:
    khadoop all restart
  
  Stop nodenamanger on an01 and an02:
    khadoop nodemanager stop an01 an02
  
  Restart worker processes (nodemanager, datanode) on worker nodes:
    khadoop workers restart
   
  Restart master processes (namenode, resourcemanager) on master node.:
    khadoop master restart
    
  tail all hadoop logs:
    khadoop logs

Note:
  This requires that you have dsh installed, and that you have a dsh 
  group named 'k' that includes all of the Kraken Analytics nodes.

"""

from docopt import docopt
import sys
import os
import time

kraken_dsh_group = 'k'
drop_killedbysig1 = True

services = {
    'namenode':          'hadoop-hdfs-namenode',
    'datanode':          'hadoop-hdfs-datanode',
    'historyserver':     'hadoop-mapreduce-historyserver',
    'resourcemanager':   'hadoop-yarn-resourcemanager',
    'nodemanager':       'hadoop-yarn-nodemanager',
    'proxyserver':       'hadoop-yarn-proxyserver',
}

master_node = 'an10'
# worker_nodes = ['an02','an03','an04','an05','an06','an07','an08','an09','an10']
worker_nodes = ['an11','an12','an13','an14','an15','an16','an17','an18','an19','an20']


def parse_dsh_nodes(nodes=None):
    if nodes:
        nodes = '-m ' + ' -m '.join(nodes)
    else:
        nodes = ' -g ' + kraken_dsh_group
    return nodes

def dsh(cmd, quiet=False):
    if not cmd.startswith('dsh '):
        cmd = 'dsh ' + cmd
    if not quiet:
        print cmd
    if drop_killedbysig1:
        cmd += " 2>&1 | grep -v -e 'Killed by signal 1.' -"
    return os.system(cmd)

def master(action):
    service('resourcemanager', action, [master_node])
    service('namenode',        action, [master_node])

def workers(action):
    service('nodemanager',  action, worker_nodes)
    service('datanode',     action, worker_nodes)

def service(name, action, nodes=None):
    dsh("%s 'sudo service %s %s'" % (parse_dsh_nodes(nodes), services[name], action))

if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)
    
    # set options
    kraken_dsh_group = arguments['--group']
    drop_killedbysig1 = not arguments['--no-drop-sig1']
    
    if arguments['logs']:
        dsh(parse_dsh_nodes(arguments['NODES']) + " 'sudo tail -f /var/log/hadoop*/*'", quiet=True)
        exit()
    
    if (arguments['<service>'] not in (services.keys() + ["all", "workers", "master"])):
        print "Error.  service must be one of " + ', '.join(services.keys() + ["all", "workers", "master"])
        exit(1)
    
    if arguments['<service>'] == 'all':
        master(arguments['<action>'])
        time.sleep(5)
        workers(arguments['<action>'])
    
    elif arguments['<service>'] == 'master':
        master(arguments['<action>'])
    
    elif arguments['<service>'] == 'workers':
        workers(arguments['<action>'])
    
    else:
        service(arguments['<service>'], arguments['<action>'], arguments['NODES'])
